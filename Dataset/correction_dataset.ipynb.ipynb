{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536e6eb-724b-40a9-b3ce-2e5c30457c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94334c7-bfa4-4f00-860d-100a552ac148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab8449-f344-4fd4-9b59-5d904aabc16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79df200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T13:09:30.132217Z",
     "start_time": "2023-09-12T13:09:21.741088Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "#from torchsummary import summary\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../helpers')\n",
    "# sys.path.insert(0,'/home/labs/rudich/sagima/helpers')\n",
    "from plotting import *\n",
    "from validate import *\n",
    "from models import *\n",
    "from data_handlers import *\n",
    "from training import *\n",
    "from metrics import *\n",
    "from explore import *\n",
    "from torchsummary import *\n",
    "# from visual_helpers_2 import *\n",
    "\n",
    "#from visual_helpers import *\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "\n",
    "#taken from the model building section - \n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 800)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "# import xgboost as xg\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import scipy \n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import torch.optim as optim\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.fft import fft, fftfreq\n",
    "import folium\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.patches import Rectangle\n",
    "import cartopy.feature\n",
    "\n",
    "\n",
    "# import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "import ast\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c5526a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T13:09:32.232514Z",
     "start_time": "2023-09-12T13:09:32.217590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>alt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4997</td>\n",
       "      <td>37.311211</td>\n",
       "      <td>27.782933</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5132</td>\n",
       "      <td>37.030961</td>\n",
       "      <td>27.971696</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5134</td>\n",
       "      <td>41.535830</td>\n",
       "      <td>20.698900</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5135</td>\n",
       "      <td>42.135560</td>\n",
       "      <td>21.714740</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5136</td>\n",
       "      <td>42.003610</td>\n",
       "      <td>21.463620</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>57645</td>\n",
       "      <td>42.533000</td>\n",
       "      <td>20.528000</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>57646</td>\n",
       "      <td>42.187769</td>\n",
       "      <td>21.030861</td>\n",
       "      <td>1803.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>72429</td>\n",
       "      <td>40.622749</td>\n",
       "      <td>40.306088</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>72510</td>\n",
       "      <td>36.302961</td>\n",
       "      <td>33.918144</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>72806</td>\n",
       "      <td>44.319164</td>\n",
       "      <td>23.796677</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>657 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     station_id        lat        lng     alt\n",
       "0          4997  37.311211  27.782933  -999.0\n",
       "1          5132  37.030961  27.971696  -999.0\n",
       "2          5134  41.535830  20.698900  -999.0\n",
       "3          5135  42.135560  21.714740  -999.0\n",
       "4          5136  42.003610  21.463620  -999.0\n",
       "..          ...        ...        ...     ...\n",
       "652       57645  42.533000  20.528000   500.0\n",
       "653       57646  42.187769  21.030861  1803.0\n",
       "654       72429  40.622749  40.306088  -999.0\n",
       "655       72510  36.302961  33.918144  -999.0\n",
       "656       72806  44.319164  23.796677  -999.0\n",
       "\n",
       "[657 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cord = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/coordinates_pm.csv\")\n",
    "df_cord = df_cord.rename(columns={df_cord.columns[0]:'station_id'})\n",
    "df_cord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac5a420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T13:25:58.837417Z",
     "start_time": "2023-09-12T13:25:55.745572Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cord = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/coordinates_pm.csv\")\n",
    "df_cord = df_cord.rename(columns={df_cord.columns[0]:'station_id'})\n",
    "df_cord\n",
    "\n",
    "df_pm0 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_0.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm1 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_1.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm2 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_2.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm3 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_3.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm4 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_4.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm5 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_5.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm6 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_6.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "\n",
    "df_pm =(\n",
    "        pd.merge(df_pm0, df_pm1, how='outer',left_index=True, right_index=True)\n",
    "        .pipe(lambda d:pd.merge(d, df_pm2, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm3, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm4, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm5, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm6, how='outer',left_index=True, right_index=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c172653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_tuple_list(string_list):\n",
    "    tuple_list = []\n",
    "    for string in string_list:\n",
    "        # Remove leading and trailing whitespace and parentheses\n",
    "        string = string.strip('()')\n",
    "        # Convert string to tuple using ast.literal_eval\n",
    "        tuple_value = ast.literal_eval(string)\n",
    "        tuple_list.append(tuple_value)\n",
    "    return tuple_list\n",
    "\n",
    "with open(\"saved_turkey_lat_lon_cords\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_lat_lon_cords = pickle.load(fp)\n",
    "\n",
    "with open(\"saved_turkey_lat_lon_idx\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_lat_lon_idx = pickle.load(fp)\n",
    "    \n",
    "with open(\"saved_greece_lat_lon_cords\", \"rb\") as fp:   # Unpickling\n",
    "    saved_greece_lat_lon_cords= pickle.load(fp)\n",
    "\n",
    "with open(\"saved_greece_lat_lon_idx\", \"rb\") as fp:   # Unpickling\n",
    "    saved_greece_lat_lon_idx = pickle.load(fp)\n",
    "    \n",
    "saved_turkey_lat_lon_cords = convert_string_to_tuple_list(saved_turkey_lat_lon_cords)\n",
    "saved_turkey_lat_lon_cords.append((35.0,27.0))\n",
    "saved_turkey_lat_lon_cords.append((35.0, 45.5))\n",
    "\n",
    "with open(\"saved_turkey_and_greece_lat_lon_cords\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_and_greece_lat_lon_cords= pickle.load(fp)\n",
    "\n",
    "with open(\"saved_turkey_and_greece_lat_lon_idx\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_and_greece_lat_lon_idx = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef56871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls /home/labs/rudich/Collaboration/dust_prediction/data/interpolated_mean_normalized/\n",
    "\n",
    "# Dnote:\n",
    "# MPM - maps of PM field (nXhXW)\n",
    "# PPM - pixels satellite (n X #pixels )\n",
    "# MPPM - average pixels satellite (n vector)\n",
    "# SPM - average over 30 stations (n vector) \n",
    "\n",
    "\n",
    "colab_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data\"\n",
    "meteo_dir = colab_dir+\"/interpolated_mean_normalized/\"\n",
    "meteo_de_dir = colab_dir+\"/meteorology_renormalization_tensors\"\n",
    "dust_dir = colab_dir+\"/dust_61368_108_2_339/\"\n",
    "\n",
    "pm10_all = torch.load(meteo_de_dir+\"/meteorology_tensors_1_81_189_interpolated_mean_normalized_pm10_tensor_denormalized.pkl\")\n",
    "labels_all = torch.load(dust_dir+\"dust_61368_108_2_339_full_tensor.pkl\")\n",
    "\n",
    "timestamps_labels = torch.load(dust_dir+\"dust_61368_108_2_339_full_timestamps.pkl\")\n",
    "timestamps_mpm = pd.to_datetime(torch.load(colab_dir+\"/meteorology_tensors_1_81_189_general_timestamps.pkl\"))\n",
    "timestamps_all = pd.to_datetime(torch.load(colab_dir+\"/meteorology_tensors_1_81_189_general_timestamps.pkl\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ab2533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data\"\n",
    "meteo_dir = colab_dir+\"/interpolated_mean_normalized/\"\n",
    "meteo_de_dir = colab_dir+\"/meteorology_renormalization_tensors\"\n",
    "dust_dir = colab_dir+\"/dust_61368_108_2_339/\"\n",
    "\n",
    "meteo_meta = torch.load(meteo_dir+\"/meteorology_tensor_62_81_189_metadata.pkl\")\n",
    "dust_meta = torch.load(dust_dir+\"/dust_61368_108_2_339_metadata.pkl\")\n",
    "\n",
    "lons_dict = meteo_meta['dims']['lons']\n",
    "lats_dict = meteo_meta['dims']['lats']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f954f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58224/2475775844.py:26: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  timestamps = df_pm.index.values.astype('datetime64[s]').astype(np.int64)\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                   | 1/2 [00:00<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_pm0 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_0.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm1 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_1.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm2 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_2.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm3 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_3.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm4 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_4.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm5 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_5.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm6 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_6.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "\n",
    "df_pm =(\n",
    "        pd.merge(df_pm0, df_pm1, how='outer',left_index=True, right_index=True)\n",
    "        .pipe(lambda d:pd.merge(d, df_pm2, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm3, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm4, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm5, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm6, how='outer',left_index=True, right_index=True))\n",
    "\n",
    "\n",
    ")\n",
    "df_cord = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/coordinates_pm.csv\")\n",
    "df_cord = df_cord.rename(columns={df_cord.columns[0]:'station_id'})\n",
    "\n",
    "\n",
    "first_year = int(df_pm.index[0][:4])\n",
    "last_year = int(df_pm.index[-1][:4])\n",
    "\n",
    "timestamps = df_pm.index.values.astype('datetime64[s]').astype(np.int64)\n",
    "stn_ids = df_pm.columns.values.astype(np.int64)\n",
    "\n",
    "\n",
    "pm10_values = df_pm.to_numpy()\n",
    "\n",
    "pm10_tensor = pm10_values.reshape(len(timestamps), len(stn_ids), 1)\n",
    "\n",
    "# Convert the tensor to a PyTorch tensor\n",
    "labels_all = torch.from_numpy(pm10_tensor)\n",
    "\n",
    "timestamp_labels = pd.to_datetime(df_pm.index)\n",
    "\n",
    "zero_var = (pm10_all.var((2,3))==0).reshape(-1).numpy()\n",
    "pm10_valid_timestamps = timestamps_all[~zero_var]\n",
    "pm10_valid_all = pm10_all[~zero_var]\n",
    "\n",
    "l = [pm10_valid_timestamps, timestamp_labels]\n",
    "shared_timestamps = l[0].intersection(l[1])\n",
    "\n",
    "shared_timestamps_idxs = []\n",
    "for timestamps_list in tqdm(l):\n",
    "    shared_idxs_per_list = []\n",
    "    for shared_t in shared_timestamps:\n",
    "        try:\n",
    "            shared_idxs_per_list.append(timestamps_list.get_loc(shared_t))\n",
    "            \n",
    "        except:\n",
    "            print(f\"Error! Something wierd happend with {shared_t} and {timestamps_list}. Aborting\")\n",
    "        if shared_t == shared_timestamps[-1]:\n",
    "            print(max(shared_idxs_per_list))\n",
    "    shared_timestamps_idxs.append(shared_idxs_per_list)\n",
    "\n",
    "inputs_all = pm10_valid_all\n",
    "labels_all = labels_all\n",
    "\n",
    "\n",
    "\n",
    "years = [ i for i in range(first_year, last_year)]\n",
    "months = [i for i in range(1,13)]\n",
    "\n",
    "dt = pd.Series(shared_timestamps).dt\n",
    "years = np.isin(dt.year, years)\n",
    "months = np.isin(dt.month, months)\n",
    "include = (years) * (months)\n",
    "\n",
    "pm10_shared_timestamps = shared_timestamps[include]\n",
    "inputs = inputs_all[shared_timestamps_idxs[0]][include]\n",
    "labels = labels_all[shared_timestamps_idxs[1]][include]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b1b7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels46 = [\"Q250\",\"Q500\",\"Q700\",\"Q850\",\"Q900\",\n",
    "            \"T250\",\"T500\",\"T700\",\"T850\",\"T900\",\n",
    "            \"U250\",\"U500\",\"U700\",\"U850\",\"U900\",\n",
    "            \"V250\",\"V500\",\"V700\",\"V850\",\"V900\",\n",
    "            \"Z250\",\"Z500\",\"Z700\",\"Z850\",\"Z900\",\n",
    "            \"PV250\",\"PV500\",\"PV700\",\"PV850\",\"PV900\",\n",
    "            \"OMEGA250\",\"OMEGA500\",\"OMEGA700\",\"OMEGA850\",\"OMEGA900\",\n",
    "            \"SLP\",\"u10\",\"v10\",\"duaod550\",'aermr06_50','aermr06_40','aermr06_30', 'aermr06_20',\n",
    "              \"pm2p5\",\"pm10\",\"tcwv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0315abc-5bc4-4fb4-b1b0-e5d9e4c36010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52514, 4, 81, 189])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensors_list = []\n",
    "pixels_list = []\n",
    "#labels_all = torch.load(dust_dir+\"dust_61368_108_2_339_full_tensor.pkl\")\n",
    "for channel in channels_for_vis:\n",
    "    data_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data/interpolated_mean_normalized/\"\n",
    "    dust_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data/dust_61368_108_2_339/\"\n",
    "    channel_all = torch.load(f\"/home/labs/rudich/Collaboration/dust_prediction/data/meteorology_renormalization_tensors/meteorology_tensors_1_81_189_interpolated_mean_normalized_{channel}_tensor_denormalized.pkl\")\n",
    "    tensors_list.append(channel_all)\n",
    "stacked_tensor = torch.stack(tensors_list, dim=1)\n",
    "stacked_tensor = stacked_tensor.squeeze(dim=2)\n",
    "stacked_tensor = stacked_tensor[~zero_var]\n",
    "print(stacked_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4af3314a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                   | 8/46 [00:27<02:12,  3.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/labs/rudich/Collaboration/dust_prediction/data/interpolated_mean_normalized/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     dust_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/labs/rudich/Collaboration/dust_prediction/data/dust_61368_108_2_339/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     channel_all \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/labs/rudich/Collaboration/dust_prediction/data/meteorology_renormalization_tensors/meteorology_tensors_1_81_189_interpolated_mean_normalized_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mchannels46\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_tensor_denormalized.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     tensors_list\u001b[38;5;241m.\u001b[39mappend(channel_all)\n\u001b[1;32m      9\u001b[0m stacked_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(tensors_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/geo-ds-env/lib/python3.9/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/geo-ds-env/lib/python3.9/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/geo-ds-env/lib/python3.9/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/.conda/envs/geo-ds-env/lib/python3.9/site-packages/torch/serialization.py:1112\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1110\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1112\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m   1116\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m         _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "tensors_list = []\n",
    "pixels_list = []\n",
    "#labels_all = torch.load(dust_dir+\"dust_61368_108_2_339_full_tensor.pkl\")\n",
    "for channel in tqdm(range(len(channels46))):\n",
    "    data_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data/interpolated_mean_normalized/\"\n",
    "    dust_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data/dust_61368_108_2_339/\"\n",
    "    channel_all = torch.load(f\"/home/labs/rudich/Collaboration/dust_prediction/data/meteorology_renormalization_tensors/meteorology_tensors_1_81_189_interpolated_mean_normalized_{channels46[channel]}_tensor_denormalized.pkl\")\n",
    "    tensors_list.append(channel_all)\n",
    "stacked_tensor = torch.stack(tensors_list, dim=1)\n",
    "stacked_tensor = stacked_tensor.squeeze(dim=2)\n",
    "stacked_tensor = stacked_tensor[~zero_var]\n",
    "print(stacked_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2100b3a-276a-48c8-a07b-d20418d06a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the slice indices\n",
    "lat_start, lat_end =  54, 66\n",
    "lon_start, lon_end = 104, 117\n",
    "\n",
    "sliced_tensor = stacked_tensor[:, :, lat_start:lat_end, lon_start:lon_end]\n",
    "\n",
    "# Create new dictionaries for the sliced lat and lon indices\n",
    "new_lats_dict = {i: lats_dict[i] for i in range(lat_start, lat_end)}\n",
    "new_lons_dict = {i: lons_dict[i] for i in range(lon_start, lon_end)}\n",
    "\n",
    "def replace_keys_with_sequential_integers(dictionary):\n",
    "    new_dict = {}\n",
    "    keys = list(dictionary.keys())\n",
    "    for i in range(len(keys)):\n",
    "        new_dict[i] = dictionary[keys[i]]\n",
    "    return new_dict\n",
    "\n",
    "new_lats_dict = replace_keys_with_sequential_integers(new_lats_dict)\n",
    "new_lons_dict = replace_keys_with_sequential_integers(new_lons_dict)\n",
    "\n",
    "del stacked_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2235aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stations_lonlat = (df_cord \n",
    "                       .rename(columns={'lng':'lon'})\n",
    "                       .assign(stn_id=np.arange(0,df_cord.shape[0],1))\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9285a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(list1, list2):\n",
    "    matrix = [[(num1, num2) for num2 in list2] for num1 in list1]\n",
    "    return matrix\n",
    "\n",
    "def calculate_distances(matrix, df):\n",
    "    lat1 = matrix[:, 0].reshape(-1, 1)\n",
    "    lon1 = matrix[:, 1].reshape(-1, 1)\n",
    "\n",
    "    lat2 = df['lat'].values.reshape(1, -1)\n",
    "    lon2 = df['lon'].values.reshape(1, -1)\n",
    "\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "\n",
    "    radius = 6371.0\n",
    "\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    dlat_sq = np.sin(dlat / 2) ** 2\n",
    "    dlon_sq = np.sin(dlon / 2) ** 2\n",
    "\n",
    "    a = dlat_sq + np.cos(lat1_rad) * np.cos(lat2_rad) * dlon_sq\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    distances = radius * c\n",
    "\n",
    "    return distances\n",
    "\n",
    "station_df = all_stations_lonlat\n",
    "pm10_df = df_pm\n",
    "\n",
    "averaged_values = {}\n",
    "\n",
    "\n",
    "matrix = create_matrix(list(lats_dict.values()), list(lons_dict.values()))\n",
    "matrix = np.array(matrix).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7991cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stations_tensor = np.zeros((pm10_valid_timestamps.shape[0], 1, len(lats_dict), len(lons_dict)))\n",
    "\n",
    "distances = cdist(matrix, station_df[['lat', 'lon']], metric='euclidean')\n",
    "result = calculate_distances(matrix, station_df[['lat', 'lon']])\n",
    "\n",
    "inv_lats = {v: k for k, v in lats_dict.items()}\n",
    "inv_lons = {v: k for k, v in lons_dict.items()}\n",
    "list_of_pixels_with_stations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9fe1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_df.index = pd.to_datetime(pm10_df.index)\n",
    "pm10_df.columns =  pm10_df.columns.astype(int)\n",
    "set1 = set(pm10_valid_timestamps)\n",
    "set2 = set(pm10_df.index)\n",
    "\n",
    "# Find the intersection of the sets\n",
    "intersection = set1.intersection(set2)\n",
    "\n",
    "# Convert the intersection set to a DatetimeIndex\n",
    "common_timestamps = pd.DatetimeIndex(pd.to_datetime(list(intersection)))\n",
    "common_timestamps = common_timestamps.sort_values()\n",
    "# Find the index where the intersection starts\n",
    "start_index = list(pm10_valid_timestamps).index(common_timestamps[0])\n",
    "\n",
    "# Find the index where the intersection ends\n",
    "end_index = list(pm10_valid_timestamps).index(common_timestamps[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "350105e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_timestamps.to_frame().to_csv('common_timestamps_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7bca9",
   "metadata": {},
   "source": [
    "# Creating a dict with pixel coordinates - stations id - distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e47b72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15309/15309 [00:02<00:00, 7171.22it/s]\n"
     ]
    }
   ],
   "source": [
    "threshold = 56\n",
    "pixels_to_stations_dict = {}\n",
    "pixels_to_stations_dict2 = {}\n",
    "for i in tqdm(range(distances.shape[0])):\n",
    "    nearby_stations = station_df[np.abs(result[i]) < threshold] #number taken using a test for the distance between the pixels\n",
    "    nearby_distances = result[i][np.abs(result[i]) < threshold]\n",
    "    if len(nearby_stations) > 0:\n",
    "\n",
    "        list_of_pixels_with_stations.append(matrix[i])\n",
    "        str_list_lat_lon = [str(i) for i in nearby_stations[['lat','lon']].values.tolist()]\n",
    "        str_list_id = nearby_stations['station_id'].values.tolist()\n",
    "        pixels_to_stations_dict[str(matrix[i])] = dict(zip(str_list_lat_lon, list(nearby_distances)))\n",
    "        pixels_to_stations_dict2[str(matrix[i])] = dict(zip(str_list_id, list(nearby_distances)))\n",
    "\n",
    "simple_dict = {key: list(inner_dict.keys()) for key, inner_dict in pixels_to_stations_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb148fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stations = df_pm.columns.tolist() #stations with data on them\n",
    "def filter_dict_by_list(dictionary, values_list):\n",
    "    # Create a new dictionary to store the filtered values\n",
    "    filtered_dict = {}\n",
    "\n",
    "    # Iterate over the dictionary items\n",
    "    for key, value_list in dictionary.items():\n",
    "        # Filter the list based on the values_list\n",
    "        filtered_list = [value for value in value_list if value in values_list]\n",
    "        \n",
    "        # Add the key-filtered_list pair to the filtered dictionary\n",
    "        filtered_dict[key] = filtered_list\n",
    "\n",
    "    return filtered_dict\n",
    "\n",
    "simple_dict = filter_dict_by_list(simple_dict,valid_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e24894bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('pixel_to_station_id.pkl','wb' ) as f:\n",
    "#     pickle.dump(simple_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "72fe4519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [1:36:49<00:00, 31.07s/it]\n"
     ]
    }
   ],
   "source": [
    "stations_tensor[:,0,:,:] = np.nan\n",
    "# Iterate over each key in the simple_dict\n",
    "for key in tqdm(simple_dict):\n",
    "    # Extract the latitude and longitude values from the key\n",
    "    values = key[1:-1].split()\n",
    "\n",
    "    # Convert the values to float\n",
    "    values = [float(value) for value in values]\n",
    "\n",
    "    lat = values[0]\n",
    "    lon = values[1]\n",
    "\n",
    "    # Get the corresponding indices from the inv_lats and inv_lons dictionaries\n",
    "    lat_index = inv_lats[lat]\n",
    "    lon_index = inv_lons[lon]\n",
    "\n",
    "    # Iterate over each timestamp in the common_timestamps list\n",
    "    for t, timestamp in enumerate(common_timestamps):\n",
    "        if t >= start_index:\n",
    "            # Get the corresponding row in the pm10_df DataFrame based on the timestamp\n",
    "            df_row = pm10_df.loc[timestamp]\n",
    "\n",
    "            # Filter the columns in the DataFrame based on the values from the simple_dict\n",
    "            filtered_df = df_row[simple_dict[key]]\n",
    "\n",
    "            # Calculate the average value of the filtered columns\n",
    "            mean_value = filtered_df.mean()\n",
    "\n",
    "            # Update the tensor at the specified indices with the mean value\n",
    "            stations_tensor[t, 0, lat_index, lon_index] = mean_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2d29a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the slice indices\n",
    "lat_start, lat_end = 50, 65\n",
    "lon_start, lon_end = 103, 154\n",
    "\n",
    "# Slice the tensor\n",
    "sliced_tensor = full[:, :, lat_start:lat_end, lon_start:lon_end]\n",
    "\n",
    "# Create new dictionaries for the sliced lat and lon indices\n",
    "new_lats_dict = {i: lats_dict[i] for i in range(lat_start, lat_end)}\n",
    "new_lons_dict = {i: lons_dict[i] for i in range(lon_start, lon_end)}\n",
    "\n",
    "def replace_keys_with_sequential_integers(dictionary):\n",
    "    new_dict = {}\n",
    "    keys = list(dictionary.keys())\n",
    "    for i in range(len(keys)):\n",
    "        new_dict[i] = dictionary[keys[i]]\n",
    "    return new_dict\n",
    "\n",
    "new_lats_dict = replace_keys_with_sequential_integers(new_lats_dict)\n",
    "new_lons_dict = replace_keys_with_sequential_integers(new_lons_dict)\n",
    "\n",
    "del full\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfb82e",
   "metadata": {},
   "source": [
    "# tensor to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6ca6301",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 20.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>time</th>\n",
       "      <th>lat_lon_coord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>221.524216</td>\n",
       "      <td>252.856476</td>\n",
       "      <td>267.699463</td>\n",
       "      <td>277.711273</td>\n",
       "      <td>280.923279</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.709136e-11</td>\n",
       "      <td>-2.025620e-12</td>\n",
       "      <td>1.387664e-11</td>\n",
       "      <td>3.461629e-11</td>\n",
       "      <td>1.464825e-08</td>\n",
       "      <td>2.722577e-08</td>\n",
       "      <td>17.485289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-01-01 00:00:00+00:00</td>\n",
       "      <td>(35.0, 19.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>222.469406</td>\n",
       "      <td>248.434280</td>\n",
       "      <td>267.991730</td>\n",
       "      <td>277.618134</td>\n",
       "      <td>280.901459</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.875626e-11</td>\n",
       "      <td>-1.081209e-12</td>\n",
       "      <td>1.330524e-11</td>\n",
       "      <td>3.313251e-11</td>\n",
       "      <td>1.773758e-08</td>\n",
       "      <td>3.374522e-08</td>\n",
       "      <td>15.694367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-01-01 03:00:00+00:00</td>\n",
       "      <td>(35.0, 19.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>222.595139</td>\n",
       "      <td>247.227905</td>\n",
       "      <td>268.481232</td>\n",
       "      <td>277.179199</td>\n",
       "      <td>280.650970</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.211665e-13</td>\n",
       "      <td>-1.367978e-13</td>\n",
       "      <td>1.273384e-11</td>\n",
       "      <td>3.164874e-11</td>\n",
       "      <td>2.082692e-08</td>\n",
       "      <td>4.026467e-08</td>\n",
       "      <td>13.903446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-01-01 06:00:00+00:00</td>\n",
       "      <td>(35.0, 19.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>223.250992</td>\n",
       "      <td>248.208664</td>\n",
       "      <td>266.922791</td>\n",
       "      <td>277.502838</td>\n",
       "      <td>280.649384</td>\n",
       "      <td>...</td>\n",
       "      <td>1.237930e-12</td>\n",
       "      <td>-6.595037e-14</td>\n",
       "      <td>4.534037e-11</td>\n",
       "      <td>3.155716e-11</td>\n",
       "      <td>2.201033e-08</td>\n",
       "      <td>4.246103e-08</td>\n",
       "      <td>14.509666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-01-01 09:00:00+00:00</td>\n",
       "      <td>(35.0, 19.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>221.955292</td>\n",
       "      <td>248.784119</td>\n",
       "      <td>267.021027</td>\n",
       "      <td>277.123169</td>\n",
       "      <td>280.224884</td>\n",
       "      <td>...</td>\n",
       "      <td>2.897027e-12</td>\n",
       "      <td>4.897048e-15</td>\n",
       "      <td>7.794691e-11</td>\n",
       "      <td>3.146559e-11</td>\n",
       "      <td>2.319373e-08</td>\n",
       "      <td>4.465739e-08</td>\n",
       "      <td>15.115888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-01-01 12:00:00+00:00</td>\n",
       "      <td>(35.0, 19.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40173205</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>218.902679</td>\n",
       "      <td>257.543121</td>\n",
       "      <td>273.394135</td>\n",
       "      <td>276.424622</td>\n",
       "      <td>279.130249</td>\n",
       "      <td>...</td>\n",
       "      <td>3.326063e-09</td>\n",
       "      <td>2.786967e-11</td>\n",
       "      <td>5.412007e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.153348e-08</td>\n",
       "      <td>1.580973e-08</td>\n",
       "      <td>2.493623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-31 09:00:00+00:00</td>\n",
       "      <td>(42.0, 44.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40173206</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>218.872070</td>\n",
       "      <td>258.164948</td>\n",
       "      <td>273.569397</td>\n",
       "      <td>276.014679</td>\n",
       "      <td>278.239471</td>\n",
       "      <td>...</td>\n",
       "      <td>2.763189e-09</td>\n",
       "      <td>3.063281e-11</td>\n",
       "      <td>6.874656e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.103926e-08</td>\n",
       "      <td>1.543707e-08</td>\n",
       "      <td>2.518091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-31 12:00:00+00:00</td>\n",
       "      <td>(42.0, 44.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40173207</th>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>218.995911</td>\n",
       "      <td>257.895905</td>\n",
       "      <td>273.583618</td>\n",
       "      <td>276.744629</td>\n",
       "      <td>278.258575</td>\n",
       "      <td>...</td>\n",
       "      <td>2.604069e-09</td>\n",
       "      <td>3.633998e-11</td>\n",
       "      <td>3.397406e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.048309e-08</td>\n",
       "      <td>1.430515e-08</td>\n",
       "      <td>2.394837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-31 15:00:00+00:00</td>\n",
       "      <td>(42.0, 44.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40173208</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>219.082367</td>\n",
       "      <td>257.514435</td>\n",
       "      <td>273.437439</td>\n",
       "      <td>277.136749</td>\n",
       "      <td>277.807831</td>\n",
       "      <td>...</td>\n",
       "      <td>2.444948e-09</td>\n",
       "      <td>4.204716e-11</td>\n",
       "      <td>-7.984380e-17</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.926912e-09</td>\n",
       "      <td>1.317322e-08</td>\n",
       "      <td>2.271582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-31 18:00:00+00:00</td>\n",
       "      <td>(42.0, 44.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40173209</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>219.054138</td>\n",
       "      <td>257.371613</td>\n",
       "      <td>273.032593</td>\n",
       "      <td>277.016693</td>\n",
       "      <td>278.434662</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464941e-09</td>\n",
       "      <td>8.484143e-11</td>\n",
       "      <td>-4.246995e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.884399e-09</td>\n",
       "      <td>1.205881e-08</td>\n",
       "      <td>2.910752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-31 21:00:00+00:00</td>\n",
       "      <td>(42.0, 44.5)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40173210 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4           5  \\\n",
       "0         0.000011  0.000138  0.002325  0.004684  0.005909  221.524216   \n",
       "1         0.000011  0.000337  0.001836  0.004204  0.005016  222.469406   \n",
       "2         0.000010  0.000242  0.000516  0.003380  0.005039  222.595139   \n",
       "3         0.000008  0.000229  0.001980  0.004509  0.005848  223.250992   \n",
       "4         0.000009  0.000145  0.001721  0.004269  0.005898  221.955292   \n",
       "...            ...       ...       ...       ...       ...         ...   \n",
       "40173205  0.000022  0.000141  0.000723  0.002411  0.002335  218.902679   \n",
       "40173206  0.000024  0.000139  0.000756  0.002895  0.002766  218.872070   \n",
       "40173207  0.000025  0.000139  0.000752  0.002678  0.003193  218.995911   \n",
       "40173208  0.000024  0.000142  0.000845  0.002460  0.003452  219.082367   \n",
       "40173209  0.000020  0.000144  0.000933  0.002419  0.003159  219.054138   \n",
       "\n",
       "                   6           7           8           9  ...            39  \\\n",
       "0         252.856476  267.699463  277.711273  280.923279  ... -7.709136e-11   \n",
       "1         248.434280  267.991730  277.618134  280.901459  ... -3.875626e-11   \n",
       "2         247.227905  268.481232  277.179199  280.650970  ... -4.211665e-13   \n",
       "3         248.208664  266.922791  277.502838  280.649384  ...  1.237930e-12   \n",
       "4         248.784119  267.021027  277.123169  280.224884  ...  2.897027e-12   \n",
       "...              ...         ...         ...         ...  ...           ...   \n",
       "40173205  257.543121  273.394135  276.424622  279.130249  ...  3.326063e-09   \n",
       "40173206  258.164948  273.569397  276.014679  278.239471  ...  2.763189e-09   \n",
       "40173207  257.895905  273.583618  276.744629  278.258575  ...  2.604069e-09   \n",
       "40173208  257.514435  273.437439  277.136749  277.807831  ...  2.444948e-09   \n",
       "40173209  257.371613  273.032593  277.016693  278.434662  ...  1.464941e-09   \n",
       "\n",
       "                    40            41            42            43  \\\n",
       "0        -2.025620e-12  1.387664e-11  3.461629e-11  1.464825e-08   \n",
       "1        -1.081209e-12  1.330524e-11  3.313251e-11  1.773758e-08   \n",
       "2        -1.367978e-13  1.273384e-11  3.164874e-11  2.082692e-08   \n",
       "3        -6.595037e-14  4.534037e-11  3.155716e-11  2.201033e-08   \n",
       "4         4.897048e-15  7.794691e-11  3.146559e-11  2.319373e-08   \n",
       "...                ...           ...           ...           ...   \n",
       "40173205  2.786967e-11  5.412007e-15  0.000000e+00  1.153348e-08   \n",
       "40173206  3.063281e-11  6.874656e-15  0.000000e+00  1.103926e-08   \n",
       "40173207  3.633998e-11  3.397406e-15  0.000000e+00  1.048309e-08   \n",
       "40173208  4.204716e-11 -7.984380e-17  0.000000e+00  9.926912e-09   \n",
       "40173209  8.484143e-11 -4.246995e-15  0.000000e+00  8.884399e-09   \n",
       "\n",
       "                    44         45  46                      time  lat_lon_coord  \n",
       "0         2.722577e-08  17.485289 NaN 2003-01-01 00:00:00+00:00   (35.0, 19.5)  \n",
       "1         3.374522e-08  15.694367 NaN 2003-01-01 03:00:00+00:00   (35.0, 19.5)  \n",
       "2         4.026467e-08  13.903446 NaN 2003-01-01 06:00:00+00:00   (35.0, 19.5)  \n",
       "3         4.246103e-08  14.509666 NaN 2003-01-01 09:00:00+00:00   (35.0, 19.5)  \n",
       "4         4.465739e-08  15.115888 NaN 2003-01-01 12:00:00+00:00   (35.0, 19.5)  \n",
       "...                ...        ...  ..                       ...            ...  \n",
       "40173205  1.580973e-08   2.493623 NaN 2020-12-31 09:00:00+00:00   (42.0, 44.5)  \n",
       "40173206  1.543707e-08   2.518091 NaN 2020-12-31 12:00:00+00:00   (42.0, 44.5)  \n",
       "40173207  1.430515e-08   2.394837 NaN 2020-12-31 15:00:00+00:00   (42.0, 44.5)  \n",
       "40173208  1.317322e-08   2.271582 NaN 2020-12-31 18:00:00+00:00   (42.0, 44.5)  \n",
       "40173209  1.205881e-08   2.910752 NaN 2020-12-31 21:00:00+00:00   (42.0, 44.5)  \n",
       "\n",
       "[40173210 rows x 49 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tensor = sliced_tensor\n",
    "# Given information\n",
    "tensor_shape = (52514, 47, 15, 51)\n",
    "\n",
    "timestamps = common_timestamps # List of timestamps (length 52514)\n",
    "\n",
    "# Reshape the tensor\n",
    "reshaped_tensor = np.reshape(tensor, (tensor_shape[0], tensor_shape[1], -1))\n",
    "\n",
    "# Initialize an empty list to store individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over lat and lon combinations\n",
    "for lat_index in tqdm(range(tensor_shape[2])):\n",
    "    for lon_index in range(tensor_shape[3]):        \n",
    "        # Create a DataFrame for the current lat-lon combination\n",
    "        df = pd.DataFrame(reshaped_tensor[:, :, (lat_index * tensor_shape[3]) + lon_index])\n",
    "\n",
    "        df['time'] = timestamps\n",
    "        lat_loc = new_lats_dict[str(lat_index)]\n",
    "        lon_loc = new_lons_dict[str(lon_index)]\n",
    "        df['lat_lon_coord'] = str((float(lat_loc),float(lon_loc)))\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all the individual DataFrames into one big DataFrame\n",
    "big_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = (big_df.set_axis(channels46 +['stn_mesm','time','lat_lon_coord'],axis=1 )\n",
    "      .drop('pm2p5',axis=1)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af27379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('lat_lon_coord')\n",
    "\n",
    "# Creating new columns with previous row values for each group\n",
    "for column in df.drop(['stn_mesm','time','lat_lon_coord'],axis=1).columns:\n",
    "    new_column_name = column + '_prev'\n",
    "    df[new_column_name] = grouped[column].shift()\n",
    "    \n",
    "df['week'] = df['time'].dt.strftime('%W')\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['month'] = df['time'].dt.month\n",
    "df['year'] = df['time'].dt.year\n",
    "df['sin'] = np.sin(df['time'].dt.dayofyear * 2 * np.pi / 365)\n",
    "df['cos'] = np.cos(df['time'].dt.dayofyear * 2 * np.pi / 365)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "136f12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_delta_columns(df):\n",
    "    channels_used =  [\"Q250\",\"Q500\",\"Q700\",\"Q850\",\"Q900\",\n",
    "            \"T250\",\"T500\",\"T700\",\"T850\",\"T900\",\n",
    "            \"U250\",\"U500\",\"U700\",\"U850\",\"U900\",\n",
    "            \"V250\",\"V500\",\"V700\",\"V850\",\"V900\",\n",
    "            \"Z250\",\"Z500\",\"Z700\",\"Z850\",\"Z900\",\n",
    "            \"PV250\",\"PV500\",\"PV700\",\"PV850\",\"PV900\",\n",
    "            \"OMEGA250\",\"OMEGA500\",\"OMEGA700\",\"OMEGA850\",\"OMEGA900\",\n",
    "            \"SLP\",\"u10\",\"v10\",\"duaod550\",'aermr06_50','aermr06_40','aermr06_30', 'aermr06_20',\n",
    "              \"tcwv\"]\n",
    "\n",
    "    # Loop over the channels names\n",
    "    for col in channels_used:\n",
    "        \n",
    "        # Calculate the delta between the column and its corresponding \"_prev\" column\n",
    "        delta_col = col + '_delta'\n",
    "        prev_col = col + '_prev'\n",
    "        df[delta_col] = df[col] - df[prev_col]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = add_delta_columns(df)\n",
    "df = df.drop(['pm10_prev'],axis=1)\n",
    "df['target'] = df['stn_mesm'] - df['pm10']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-ds-env",
   "language": "python",
   "name": "geo-ds-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
