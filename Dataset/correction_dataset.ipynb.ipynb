{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536e6eb-724b-40a9-b3ce-2e5c30457c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94334c7-bfa4-4f00-860d-100a552ac148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab8449-f344-4fd4-9b59-5d904aabc16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79df200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T13:09:30.132217Z",
     "start_time": "2023-09-12T13:09:21.741088Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "#from torchsummary import summary\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../helpers')\n",
    "# sys.path.insert(0,'/home/labs/rudich/sagima/helpers')\n",
    "from plotting import *\n",
    "from validate import *\n",
    "from models import *\n",
    "from data_handlers import *\n",
    "from training import *\n",
    "from metrics import *\n",
    "from explore import *\n",
    "from torchsummary import *\n",
    "# from visual_helpers_2 import *\n",
    "\n",
    "#from visual_helpers import *\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "\n",
    "#taken from the model building section - \n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 800)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "# import xgboost as xg\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import scipy \n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import torch.optim as optim\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.fft import fft, fftfreq\n",
    "import folium\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.patches import Rectangle\n",
    "import cartopy.feature\n",
    "\n",
    "\n",
    "# import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "import ast\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c5526a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T13:09:32.232514Z",
     "start_time": "2023-09-12T13:09:32.217590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>alt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4997</td>\n",
       "      <td>37.311211</td>\n",
       "      <td>27.782933</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5132</td>\n",
       "      <td>37.030961</td>\n",
       "      <td>27.971696</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5134</td>\n",
       "      <td>41.535830</td>\n",
       "      <td>20.698900</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5135</td>\n",
       "      <td>42.135560</td>\n",
       "      <td>21.714740</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5136</td>\n",
       "      <td>42.003610</td>\n",
       "      <td>21.463620</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>57645</td>\n",
       "      <td>42.533000</td>\n",
       "      <td>20.528000</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>57646</td>\n",
       "      <td>42.187769</td>\n",
       "      <td>21.030861</td>\n",
       "      <td>1803.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>72429</td>\n",
       "      <td>40.622749</td>\n",
       "      <td>40.306088</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>72510</td>\n",
       "      <td>36.302961</td>\n",
       "      <td>33.918144</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>72806</td>\n",
       "      <td>44.319164</td>\n",
       "      <td>23.796677</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>657 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     station_id        lat        lng     alt\n",
       "0          4997  37.311211  27.782933  -999.0\n",
       "1          5132  37.030961  27.971696  -999.0\n",
       "2          5134  41.535830  20.698900  -999.0\n",
       "3          5135  42.135560  21.714740  -999.0\n",
       "4          5136  42.003610  21.463620  -999.0\n",
       "..          ...        ...        ...     ...\n",
       "652       57645  42.533000  20.528000   500.0\n",
       "653       57646  42.187769  21.030861  1803.0\n",
       "654       72429  40.622749  40.306088  -999.0\n",
       "655       72510  36.302961  33.918144  -999.0\n",
       "656       72806  44.319164  23.796677  -999.0\n",
       "\n",
       "[657 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cord = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/coordinates_pm.csv\")\n",
    "df_cord = df_cord.rename(columns={df_cord.columns[0]:'station_id'})\n",
    "df_cord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac5a420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T13:25:58.837417Z",
     "start_time": "2023-09-12T13:25:55.745572Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cord = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/coordinates_pm.csv\")\n",
    "df_cord = df_cord.rename(columns={df_cord.columns[0]:'station_id'})\n",
    "df_cord\n",
    "\n",
    "df_pm0 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_0.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm1 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_1.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm2 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_2.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm3 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_3.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm4 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_4.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm5 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_5.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm6 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_6.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "\n",
    "df_pm =(\n",
    "        pd.merge(df_pm0, df_pm1, how='outer',left_index=True, right_index=True)\n",
    "        .pipe(lambda d:pd.merge(d, df_pm2, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm3, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm4, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm5, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm6, how='outer',left_index=True, right_index=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c172653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_tuple_list(string_list):\n",
    "    tuple_list = []\n",
    "    for string in string_list:\n",
    "        # Remove leading and trailing whitespace and parentheses\n",
    "        string = string.strip('()')\n",
    "        # Convert string to tuple using ast.literal_eval\n",
    "        tuple_value = ast.literal_eval(string)\n",
    "        tuple_list.append(tuple_value)\n",
    "    return tuple_list\n",
    "\n",
    "with open(\"saved_turkey_lat_lon_cords\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_lat_lon_cords = pickle.load(fp)\n",
    "\n",
    "with open(\"saved_turkey_lat_lon_idx\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_lat_lon_idx = pickle.load(fp)\n",
    "    \n",
    "with open(\"saved_greece_lat_lon_cords\", \"rb\") as fp:   # Unpickling\n",
    "    saved_greece_lat_lon_cords= pickle.load(fp)\n",
    "\n",
    "with open(\"saved_greece_lat_lon_idx\", \"rb\") as fp:   # Unpickling\n",
    "    saved_greece_lat_lon_idx = pickle.load(fp)\n",
    "    \n",
    "saved_turkey_lat_lon_cords = convert_string_to_tuple_list(saved_turkey_lat_lon_cords)\n",
    "saved_turkey_lat_lon_cords.append((35.0,27.0))\n",
    "saved_turkey_lat_lon_cords.append((35.0, 45.5))\n",
    "\n",
    "with open(\"saved_turkey_and_greece_lat_lon_cords\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_and_greece_lat_lon_cords= pickle.load(fp)\n",
    "\n",
    "with open(\"saved_turkey_and_greece_lat_lon_idx\", \"rb\") as fp:   # Unpickling\n",
    "    saved_turkey_and_greece_lat_lon_idx = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef56871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls /home/labs/rudich/Collaboration/dust_prediction/data/interpolated_mean_normalized/\n",
    "\n",
    "# Dnote:\n",
    "# MPM - maps of PM field (nXhXW)\n",
    "# PPM - pixels satellite (n X #pixels )\n",
    "# MPPM - average pixels satellite (n vector)\n",
    "# SPM - average over 30 stations (n vector) \n",
    "\n",
    "\n",
    "colab_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data\"\n",
    "meteo_dir = colab_dir+\"/interpolated_mean_normalized/\"\n",
    "meteo_de_dir = colab_dir+\"/meteorology_renormalization_tensors\"\n",
    "dust_dir = colab_dir+\"/dust_61368_108_2_339/\"\n",
    "\n",
    "pm10_all = torch.load(meteo_de_dir+\"/meteorology_tensors_1_81_189_interpolated_mean_normalized_pm10_tensor_denormalized.pkl\")\n",
    "labels_all = torch.load(dust_dir+\"dust_61368_108_2_339_full_tensor.pkl\")\n",
    "\n",
    "timestamps_labels = torch.load(dust_dir+\"dust_61368_108_2_339_full_timestamps.pkl\")\n",
    "timestamps_mpm = pd.to_datetime(torch.load(colab_dir+\"/meteorology_tensors_1_81_189_general_timestamps.pkl\"))\n",
    "timestamps_all = pd.to_datetime(torch.load(colab_dir+\"/meteorology_tensors_1_81_189_general_timestamps.pkl\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ab2533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data\"\n",
    "meteo_dir = colab_dir+\"/interpolated_mean_normalized/\"\n",
    "meteo_de_dir = colab_dir+\"/meteorology_renormalization_tensors\"\n",
    "dust_dir = colab_dir+\"/dust_61368_108_2_339/\"\n",
    "\n",
    "meteo_meta = torch.load(meteo_dir+\"/meteorology_tensor_62_81_189_metadata.pkl\")\n",
    "dust_meta = torch.load(dust_dir+\"/dust_61368_108_2_339_metadata.pkl\")\n",
    "\n",
    "lons_dict = meteo_meta['dims']['lons']\n",
    "lats_dict = meteo_meta['dims']['lats']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f954f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58224/2475775844.py:26: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  timestamps = df_pm.index.values.astype('datetime64[s]').astype(np.int64)\n",
      " 50%|███████████████████████████████████████████████████                                                   | 1/2 [00:00<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_pm0 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_0.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm1 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_1.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm2 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_2.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm3 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_3.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm4 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_4.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm5 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_5.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "df_pm6 = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/pm_6.csv\",index_col=0).dropna(axis=1,how='all')\n",
    "\n",
    "df_pm =(\n",
    "        pd.merge(df_pm0, df_pm1, how='outer',left_index=True, right_index=True)\n",
    "        .pipe(lambda d:pd.merge(d, df_pm2, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm3, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm4, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm5, how='outer',left_index=True, right_index=True))\n",
    "        .pipe(lambda d:pd.merge(d, df_pm6, how='outer',left_index=True, right_index=True))\n",
    "\n",
    "\n",
    ")\n",
    "df_cord = pd.read_csv(\"/home/labs/rudich/sagima/TOAR/coordinates_pm.csv\")\n",
    "df_cord = df_cord.rename(columns={df_cord.columns[0]:'station_id'})\n",
    "\n",
    "\n",
    "first_year = int(df_pm.index[0][:4])\n",
    "last_year = int(df_pm.index[-1][:4])\n",
    "\n",
    "timestamps = df_pm.index.values.astype('datetime64[s]').astype(np.int64)\n",
    "stn_ids = df_pm.columns.values.astype(np.int64)\n",
    "\n",
    "\n",
    "pm10_values = df_pm.to_numpy()\n",
    "\n",
    "pm10_tensor = pm10_values.reshape(len(timestamps), len(stn_ids), 1)\n",
    "\n",
    "# Convert the tensor to a PyTorch tensor\n",
    "labels_all = torch.from_numpy(pm10_tensor)\n",
    "\n",
    "timestamp_labels = pd.to_datetime(df_pm.index)\n",
    "\n",
    "zero_var = (pm10_all.var((2,3))==0).reshape(-1).numpy()\n",
    "pm10_valid_timestamps = timestamps_all[~zero_var]\n",
    "pm10_valid_all = pm10_all[~zero_var]\n",
    "\n",
    "l = [pm10_valid_timestamps, timestamp_labels]\n",
    "shared_timestamps = l[0].intersection(l[1])\n",
    "\n",
    "shared_timestamps_idxs = []\n",
    "for timestamps_list in tqdm(l):\n",
    "    shared_idxs_per_list = []\n",
    "    for shared_t in shared_timestamps:\n",
    "        try:\n",
    "            shared_idxs_per_list.append(timestamps_list.get_loc(shared_t))\n",
    "            \n",
    "        except:\n",
    "            print(f\"Error! Something wierd happend with {shared_t} and {timestamps_list}. Aborting\")\n",
    "        if shared_t == shared_timestamps[-1]:\n",
    "            print(max(shared_idxs_per_list))\n",
    "    shared_timestamps_idxs.append(shared_idxs_per_list)\n",
    "\n",
    "inputs_all = pm10_valid_all\n",
    "labels_all = labels_all\n",
    "\n",
    "\n",
    "\n",
    "years = [ i for i in range(first_year, last_year)]\n",
    "months = [i for i in range(1,13)]\n",
    "\n",
    "dt = pd.Series(shared_timestamps).dt\n",
    "years = np.isin(dt.year, years)\n",
    "months = np.isin(dt.month, months)\n",
    "include = (years) * (months)\n",
    "\n",
    "pm10_shared_timestamps = shared_timestamps[include]\n",
    "inputs = inputs_all[shared_timestamps_idxs[0]][include]\n",
    "labels = labels_all[shared_timestamps_idxs[1]][include]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b1b7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels46 = [\"Q250\",\"Q500\",\"Q700\",\"Q850\",\"Q900\",\n",
    "            \"T250\",\"T500\",\"T700\",\"T850\",\"T900\",\n",
    "            \"U250\",\"U500\",\"U700\",\"U850\",\"U900\",\n",
    "            \"V250\",\"V500\",\"V700\",\"V850\",\"V900\",\n",
    "            \"Z250\",\"Z500\",\"Z700\",\"Z850\",\"Z900\",\n",
    "            \"PV250\",\"PV500\",\"PV700\",\"PV850\",\"PV900\",\n",
    "            \"OMEGA250\",\"OMEGA500\",\"OMEGA700\",\"OMEGA850\",\"OMEGA900\",\n",
    "            \"SLP\",\"u10\",\"v10\",\"duaod550\",'aermr06_50','aermr06_40','aermr06_30', 'aermr06_20',\n",
    "              \"pm2p5\",\"pm10\",\"tcwv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0315abc-5bc4-4fb4-b1b0-e5d9e4c36010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52514, 4, 81, 189])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensors_list = []\n",
    "pixels_list = []\n",
    "#labels_all = torch.load(dust_dir+\"dust_61368_108_2_339_full_tensor.pkl\")\n",
    "for channel in channels_for_vis:\n",
    "    data_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data/interpolated_mean_normalized/\"\n",
    "    dust_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data/dust_61368_108_2_339/\"\n",
    "    channel_all = torch.load(f\"/home/labs/rudich/Collaboration/dust_prediction/data/meteorology_renormalization_tensors/meteorology_tensors_1_81_189_interpolated_mean_normalized_{channel}_tensor_denormalized.pkl\")\n",
    "    tensors_list.append(channel_all)\n",
    "stacked_tensor = torch.stack(tensors_list, dim=1)\n",
    "stacked_tensor = stacked_tensor.squeeze(dim=2)\n",
    "stacked_tensor = stacked_tensor[~zero_var]\n",
    "print(stacked_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4af3314a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████████▌                                                                                   | 8/46 [00:27<02:12,  3.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/labs/rudich/Collaboration/dust_prediction/data/interpolated_mean_normalized/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     dust_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/labs/rudich/Collaboration/dust_prediction/data/dust_61368_108_2_339/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     channel_all \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/labs/rudich/Collaboration/dust_prediction/data/meteorology_renormalization_tensors/meteorology_tensors_1_81_189_interpolated_mean_normalized_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mchannels46\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_tensor_denormalized.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     tensors_list\u001b[38;5;241m.\u001b[39mappend(channel_all)\n\u001b[1;32m      9\u001b[0m stacked_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(tensors_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/geo-ds-env/lib/python3.9/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/geo-ds-env/lib/python3.9/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/geo-ds-env/lib/python3.9/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/.conda/envs/geo-ds-env/lib/python3.9/site-packages/torch/serialization.py:1112\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1110\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1112\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m   1116\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m         _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "tensors_list = []\n",
    "pixels_list = []\n",
    "#labels_all = torch.load(dust_dir+\"dust_61368_108_2_339_full_tensor.pkl\")\n",
    "for channel in tqdm(range(len(channels46))):\n",
    "    data_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data/interpolated_mean_normalized/\"\n",
    "    dust_dir = \"/home/labs/rudich/Collaboration/dust_prediction/data/dust_61368_108_2_339/\"\n",
    "    channel_all = torch.load(f\"/home/labs/rudich/Collaboration/dust_prediction/data/meteorology_renormalization_tensors/meteorology_tensors_1_81_189_interpolated_mean_normalized_{channels46[channel]}_tensor_denormalized.pkl\")\n",
    "    tensors_list.append(channel_all)\n",
    "stacked_tensor = torch.stack(tensors_list, dim=1)\n",
    "stacked_tensor = stacked_tensor.squeeze(dim=2)\n",
    "stacked_tensor = stacked_tensor[~zero_var]\n",
    "print(stacked_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2100b3a-276a-48c8-a07b-d20418d06a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the slice indices\n",
    "lat_start, lat_end =  54, 66\n",
    "lon_start, lon_end = 104, 117\n",
    "\n",
    "sliced_tensor = stacked_tensor[:, :, lat_start:lat_end, lon_start:lon_end]\n",
    "\n",
    "# Create new dictionaries for the sliced lat and lon indices\n",
    "new_lats_dict = {i: lats_dict[i] for i in range(lat_start, lat_end)}\n",
    "new_lons_dict = {i: lons_dict[i] for i in range(lon_start, lon_end)}\n",
    "\n",
    "def replace_keys_with_sequential_integers(dictionary):\n",
    "    new_dict = {}\n",
    "    keys = list(dictionary.keys())\n",
    "    for i in range(len(keys)):\n",
    "        new_dict[i] = dictionary[keys[i]]\n",
    "    return new_dict\n",
    "\n",
    "new_lats_dict = replace_keys_with_sequential_integers(new_lats_dict)\n",
    "new_lons_dict = replace_keys_with_sequential_integers(new_lons_dict)\n",
    "\n",
    "del stacked_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2235aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stations_lonlat = (df_cord \n",
    "                       .rename(columns={'lng':'lon'})\n",
    "                       .assign(stn_id=np.arange(0,df_cord.shape[0],1))\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9285a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(list1, list2):\n",
    "    matrix = [[(num1, num2) for num2 in list2] for num1 in list1]\n",
    "    return matrix\n",
    "\n",
    "def calculate_distances(matrix, df):\n",
    "    lat1 = matrix[:, 0].reshape(-1, 1)\n",
    "    lon1 = matrix[:, 1].reshape(-1, 1)\n",
    "\n",
    "    lat2 = df['lat'].values.reshape(1, -1)\n",
    "    lon2 = df['lon'].values.reshape(1, -1)\n",
    "\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "\n",
    "    radius = 6371.0\n",
    "\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    dlat_sq = np.sin(dlat / 2) ** 2\n",
    "    dlon_sq = np.sin(dlon / 2) ** 2\n",
    "\n",
    "    a = dlat_sq + np.cos(lat1_rad) * np.cos(lat2_rad) * dlon_sq\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    distances = radius * c\n",
    "\n",
    "    return distances\n",
    "\n",
    "station_df = all_stations_lonlat\n",
    "pm10_df = df_pm\n",
    "\n",
    "averaged_values = {}\n",
    "\n",
    "\n",
    "matrix = create_matrix(list(lats_dict.values()), list(lons_dict.values()))\n",
    "matrix = np.array(matrix).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7991cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stations_tensor = np.zeros((pm10_valid_timestamps.shape[0], 1, len(lats_dict), len(lons_dict)))\n",
    "\n",
    "distances = cdist(matrix, station_df[['lat', 'lon']], metric='euclidean')\n",
    "result = calculate_distances(matrix, station_df[['lat', 'lon']])\n",
    "\n",
    "inv_lats = {v: k for k, v in lats_dict.items()}\n",
    "inv_lons = {v: k for k, v in lons_dict.items()}\n",
    "list_of_pixels_with_stations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9fe1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_df.index = pd.to_datetime(pm10_df.index)\n",
    "pm10_df.columns =  pm10_df.columns.astype(int)\n",
    "set1 = set(pm10_valid_timestamps)\n",
    "set2 = set(pm10_df.index)\n",
    "\n",
    "# Find the intersection of the sets\n",
    "intersection = set1.intersection(set2)\n",
    "\n",
    "# Convert the intersection set to a DatetimeIndex\n",
    "common_timestamps = pd.DatetimeIndex(pd.to_datetime(list(intersection)))\n",
    "common_timestamps = common_timestamps.sort_values()\n",
    "# Find the index where the intersection starts\n",
    "start_index = list(pm10_valid_timestamps).index(common_timestamps[0])\n",
    "\n",
    "# Find the index where the intersection ends\n",
    "end_index = list(pm10_valid_timestamps).index(common_timestamps[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "350105e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_timestamps.to_frame().to_csv('common_timestamps_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7bca9",
   "metadata": {},
   "source": [
    "# Creating a dict with pixel coordinates - stations id - distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e47b72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 15309/15309 [00:02<00:00, 7171.22it/s]\n"
     ]
    }
   ],
   "source": [
    "threshold = 56\n",
    "pixels_to_stations_dict = {}\n",
    "pixels_to_stations_dict2 = {}\n",
    "for i in tqdm(range(distances.shape[0])):\n",
    "    nearby_stations = station_df[np.abs(result[i]) < threshold] #number taken using a test for the distance between the pixels\n",
    "    nearby_distances = result[i][np.abs(result[i]) < threshold]\n",
    "    if len(nearby_stations) > 0:\n",
    "\n",
    "        list_of_pixels_with_stations.append(matrix[i])\n",
    "        str_list_lat_lon = [str(i) for i in nearby_stations[['lat','lon']].values.tolist()]\n",
    "        str_list_id = nearby_stations['station_id'].values.tolist()\n",
    "        pixels_to_stations_dict[str(matrix[i])] = dict(zip(str_list_lat_lon, list(nearby_distances)))\n",
    "        pixels_to_stations_dict2[str(matrix[i])] = dict(zip(str_list_id, list(nearby_distances)))\n",
    "\n",
    "simple_dict = {key: list(inner_dict.keys()) for key, inner_dict in pixels_to_stations_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb148fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stations = df_pm.columns.tolist() #stations with data on them\n",
    "def filter_dict_by_list(dictionary, values_list):\n",
    "    # Create a new dictionary to store the filtered values\n",
    "    filtered_dict = {}\n",
    "\n",
    "    # Iterate over the dictionary items\n",
    "    for key, value_list in dictionary.items():\n",
    "        # Filter the list based on the values_list\n",
    "        filtered_list = [value for value in value_list if value in values_list]\n",
    "        \n",
    "        # Add the key-filtered_list pair to the filtered dictionary\n",
    "        filtered_dict[key] = filtered_list\n",
    "\n",
    "    return filtered_dict\n",
    "\n",
    "simple_dict = filter_dict_by_list(simple_dict,valid_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e24894bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('pixel_to_station_id.pkl','wb' ) as f:\n",
    "#     pickle.dump(simple_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "72fe4519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 187/187 [1:36:49<00:00, 31.07s/it]\n"
     ]
    }
   ],
   "source": [
    "stations_tensor[:,0,:,:] = np.nan\n",
    "# Iterate over each key in the simple_dict\n",
    "for key in tqdm(simple_dict):\n",
    "    # Extract the latitude and longitude values from the key\n",
    "    values = key[1:-1].split()\n",
    "\n",
    "    # Convert the values to float\n",
    "    values = [float(value) for value in values]\n",
    "\n",
    "    lat = values[0]\n",
    "    lon = values[1]\n",
    "\n",
    "    # Get the corresponding indices from the inv_lats and inv_lons dictionaries\n",
    "    lat_index = inv_lats[lat]\n",
    "    lon_index = inv_lons[lon]\n",
    "\n",
    "    # Iterate over each timestamp in the common_timestamps list\n",
    "    for t, timestamp in enumerate(common_timestamps):\n",
    "        if t >= start_index:\n",
    "            # Get the corresponding row in the pm10_df DataFrame based on the timestamp\n",
    "            df_row = pm10_df.loc[timestamp]\n",
    "\n",
    "            # Filter the columns in the DataFrame based on the values from the simple_dict\n",
    "            filtered_df = df_row[simple_dict[key]]\n",
    "\n",
    "            # Calculate the average value of the filtered columns\n",
    "            mean_value = filtered_df.mean()\n",
    "\n",
    "            # Update the tensor at the specified indices with the mean value\n",
    "            stations_tensor[t, 0, lat_index, lon_index] = mean_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2d29a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the slice indices\n",
    "lat_start, lat_end = 50, 65\n",
    "lon_start, lon_end = 103, 154\n",
    "\n",
    "# Slice the tensor\n",
    "sliced_tensor = full[:, :, lat_start:lat_end, lon_start:lon_end]\n",
    "\n",
    "# Create new dictionaries for the sliced lat and lon indices\n",
    "new_lats_dict = {i: lats_dict[i] for i in range(lat_start, lat_end)}\n",
    "new_lons_dict = {i: lons_dict[i] for i in range(lon_start, lon_end)}\n",
    "\n",
    "def replace_keys_with_sequential_integers(dictionary):\n",
    "    new_dict = {}\n",
    "    keys = list(dictionary.keys())\n",
    "    for i in range(len(keys)):\n",
    "        new_dict[i] = dictionary[keys[i]]\n",
    "    return new_dict\n",
    "\n",
    "new_lats_dict = replace_keys_with_sequential_integers(new_lats_dict)\n",
    "new_lons_dict = replace_keys_with_sequential_integers(new_lons_dict)\n",
    "\n",
    "del full\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfb82e",
   "metadata": {},
   "source": [
    "# tensor to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6ca6301",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 20.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>time</th>\n",
       "      <th>lat_lon_coord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>221.524216</td>\n",
       "      <td>252.856476</td>\n",
       "      <td>267.699463</td>\n",
       "      <td>277.711273</td>\n",
       "      <td>280.923279</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.709136e-11</td>\n",
       "      <td>-2.025620e-12</td>\n",
       "      <td>1.387664e-11</td>\n",
       "      <td>3.461629e-11</td>\n",
       "      <td>1.464825e-08</td>\n",
       "      <td>2.722577e-08</td>\n",
       "      <td>17.485289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-01-01 00:00:00+00:00</td>\n",
       "      <td>(35.0, 19.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>222.469406</td>\n",
       "      <td>248.434280</td>\n",
       "      <td>267.991730</td>\n",
       "      <td>277.618134</td>\n",
       "      <td>280.901459</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.875626e-11</td>\n",
       "      <td>-1.081209e-12</td>\n",
       "      <td>1.330524e-11</td>\n",
       "      <td>3.313251e-11</td>\n",
       "      <td>1.773758e-08</td>\n",
       "      <td>3.374522e-08</td>\n",
       "      <td>15.694367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-01-01 03:00:00+00:00</td>\n",
       "      <td>(35.0, 19.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>222.595139</td>\n",
       "      <td>247.227905</td>\n",
       "      <td>268.481232</td>\n",
       "      <td>277.179199</td>\n",
       "      <td>280.650970</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.211665e-13</td>\n",
       "      <td>-1.367978e-13</td>\n",
       "      <td>1.273384e-11</td>\n",
       "      <td>3.164874e-11</td>\n",
       "      <td>2.082692e-08</td>\n",
       "      <td>4.026467e-08</td>\n",
       "      <td>13.903446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-01-01 06:00:00+00:00</td>\n",
       "      <td>(35.0, 19.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>223.250992</td>\n",
       "      <td>248.208664</td>\n",
       "      <td>266.922791</td>\n",
       "      <td>277.502838</td>\n",
       "      <td>280.649384</td>\n",
       "      <td>...</td>\n",
       "      <td>1.237930e-12</td>\n",
       "      <td>-6.595037e-14</td>\n",
       "      <td>4.534037e-11</td>\n",
       "      <td>3.155716e-11</td>\n",
       "      <td>2.201033e-08</td>\n",
       "      <td>4.246103e-08</td>\n",
       "      <td>14.509666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-01-01 09:00:00+00:00</td>\n",
       "      <td>(35.0, 19.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>221.955292</td>\n",
       "      <td>248.784119</td>\n",
       "      <td>267.021027</td>\n",
       "      <td>277.123169</td>\n",
       "      <td>280.224884</td>\n",
       "      <td>...</td>\n",
       "      <td>2.897027e-12</td>\n",
       "      <td>4.897048e-15</td>\n",
       "      <td>7.794691e-11</td>\n",
       "      <td>3.146559e-11</td>\n",
       "      <td>2.319373e-08</td>\n",
       "      <td>4.465739e-08</td>\n",
       "      <td>15.115888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-01-01 12:00:00+00:00</td>\n",
       "      <td>(35.0, 19.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40173205</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>218.902679</td>\n",
       "      <td>257.543121</td>\n",
       "      <td>273.394135</td>\n",
       "      <td>276.424622</td>\n",
       "      <td>279.130249</td>\n",
       "      <td>...</td>\n",
       "      <td>3.326063e-09</td>\n",
       "      <td>2.786967e-11</td>\n",
       "      <td>5.412007e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.153348e-08</td>\n",
       "      <td>1.580973e-08</td>\n",
       "      <td>2.493623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-31 09:00:00+00:00</td>\n",
       "      <td>(42.0, 44.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40173206</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>218.872070</td>\n",
       "      <td>258.164948</td>\n",
       "      <td>273.569397</td>\n",
       "      <td>276.014679</td>\n",
       "      <td>278.239471</td>\n",
       "      <td>...</td>\n",
       "      <td>2.763189e-09</td>\n",
       "      <td>3.063281e-11</td>\n",
       "      <td>6.874656e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.103926e-08</td>\n",
       "      <td>1.543707e-08</td>\n",
       "      <td>2.518091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-31 12:00:00+00:00</td>\n",
       "      <td>(42.0, 44.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40173207</th>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>218.995911</td>\n",
       "      <td>257.895905</td>\n",
       "      <td>273.583618</td>\n",
       "      <td>276.744629</td>\n",
       "      <td>278.258575</td>\n",
       "      <td>...</td>\n",
       "      <td>2.604069e-09</td>\n",
       "      <td>3.633998e-11</td>\n",
       "      <td>3.397406e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.048309e-08</td>\n",
       "      <td>1.430515e-08</td>\n",
       "      <td>2.394837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-31 15:00:00+00:00</td>\n",
       "      <td>(42.0, 44.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40173208</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>219.082367</td>\n",
       "      <td>257.514435</td>\n",
       "      <td>273.437439</td>\n",
       "      <td>277.136749</td>\n",
       "      <td>277.807831</td>\n",
       "      <td>...</td>\n",
       "      <td>2.444948e-09</td>\n",
       "      <td>4.204716e-11</td>\n",
       "      <td>-7.984380e-17</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.926912e-09</td>\n",
       "      <td>1.317322e-08</td>\n",
       "      <td>2.271582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-31 18:00:00+00:00</td>\n",
       "      <td>(42.0, 44.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40173209</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>219.054138</td>\n",
       "      <td>257.371613</td>\n",
       "      <td>273.032593</td>\n",
       "      <td>277.016693</td>\n",
       "      <td>278.434662</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464941e-09</td>\n",
       "      <td>8.484143e-11</td>\n",
       "      <td>-4.246995e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.884399e-09</td>\n",
       "      <td>1.205881e-08</td>\n",
       "      <td>2.910752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-31 21:00:00+00:00</td>\n",
       "      <td>(42.0, 44.5)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40173210 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4           5  \\\n",
       "0         0.000011  0.000138  0.002325  0.004684  0.005909  221.524216   \n",
       "1         0.000011  0.000337  0.001836  0.004204  0.005016  222.469406   \n",
       "2         0.000010  0.000242  0.000516  0.003380  0.005039  222.595139   \n",
       "3         0.000008  0.000229  0.001980  0.004509  0.005848  223.250992   \n",
       "4         0.000009  0.000145  0.001721  0.004269  0.005898  221.955292   \n",
       "...            ...       ...       ...       ...       ...         ...   \n",
       "40173205  0.000022  0.000141  0.000723  0.002411  0.002335  218.902679   \n",
       "40173206  0.000024  0.000139  0.000756  0.002895  0.002766  218.872070   \n",
       "40173207  0.000025  0.000139  0.000752  0.002678  0.003193  218.995911   \n",
       "40173208  0.000024  0.000142  0.000845  0.002460  0.003452  219.082367   \n",
       "40173209  0.000020  0.000144  0.000933  0.002419  0.003159  219.054138   \n",
       "\n",
       "                   6           7           8           9  ...            39  \\\n",
       "0         252.856476  267.699463  277.711273  280.923279  ... -7.709136e-11   \n",
       "1         248.434280  267.991730  277.618134  280.901459  ... -3.875626e-11   \n",
       "2         247.227905  268.481232  277.179199  280.650970  ... -4.211665e-13   \n",
       "3         248.208664  266.922791  277.502838  280.649384  ...  1.237930e-12   \n",
       "4         248.784119  267.021027  277.123169  280.224884  ...  2.897027e-12   \n",
       "...              ...         ...         ...         ...  ...           ...   \n",
       "40173205  257.543121  273.394135  276.424622  279.130249  ...  3.326063e-09   \n",
       "40173206  258.164948  273.569397  276.014679  278.239471  ...  2.763189e-09   \n",
       "40173207  257.895905  273.583618  276.744629  278.258575  ...  2.604069e-09   \n",
       "40173208  257.514435  273.437439  277.136749  277.807831  ...  2.444948e-09   \n",
       "40173209  257.371613  273.032593  277.016693  278.434662  ...  1.464941e-09   \n",
       "\n",
       "                    40            41            42            43  \\\n",
       "0        -2.025620e-12  1.387664e-11  3.461629e-11  1.464825e-08   \n",
       "1        -1.081209e-12  1.330524e-11  3.313251e-11  1.773758e-08   \n",
       "2        -1.367978e-13  1.273384e-11  3.164874e-11  2.082692e-08   \n",
       "3        -6.595037e-14  4.534037e-11  3.155716e-11  2.201033e-08   \n",
       "4         4.897048e-15  7.794691e-11  3.146559e-11  2.319373e-08   \n",
       "...                ...           ...           ...           ...   \n",
       "40173205  2.786967e-11  5.412007e-15  0.000000e+00  1.153348e-08   \n",
       "40173206  3.063281e-11  6.874656e-15  0.000000e+00  1.103926e-08   \n",
       "40173207  3.633998e-11  3.397406e-15  0.000000e+00  1.048309e-08   \n",
       "40173208  4.204716e-11 -7.984380e-17  0.000000e+00  9.926912e-09   \n",
       "40173209  8.484143e-11 -4.246995e-15  0.000000e+00  8.884399e-09   \n",
       "\n",
       "                    44         45  46                      time  lat_lon_coord  \n",
       "0         2.722577e-08  17.485289 NaN 2003-01-01 00:00:00+00:00   (35.0, 19.5)  \n",
       "1         3.374522e-08  15.694367 NaN 2003-01-01 03:00:00+00:00   (35.0, 19.5)  \n",
       "2         4.026467e-08  13.903446 NaN 2003-01-01 06:00:00+00:00   (35.0, 19.5)  \n",
       "3         4.246103e-08  14.509666 NaN 2003-01-01 09:00:00+00:00   (35.0, 19.5)  \n",
       "4         4.465739e-08  15.115888 NaN 2003-01-01 12:00:00+00:00   (35.0, 19.5)  \n",
       "...                ...        ...  ..                       ...            ...  \n",
       "40173205  1.580973e-08   2.493623 NaN 2020-12-31 09:00:00+00:00   (42.0, 44.5)  \n",
       "40173206  1.543707e-08   2.518091 NaN 2020-12-31 12:00:00+00:00   (42.0, 44.5)  \n",
       "40173207  1.430515e-08   2.394837 NaN 2020-12-31 15:00:00+00:00   (42.0, 44.5)  \n",
       "40173208  1.317322e-08   2.271582 NaN 2020-12-31 18:00:00+00:00   (42.0, 44.5)  \n",
       "40173209  1.205881e-08   2.910752 NaN 2020-12-31 21:00:00+00:00   (42.0, 44.5)  \n",
       "\n",
       "[40173210 rows x 49 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tensor = sliced_tensor\n",
    "# Given information\n",
    "tensor_shape = (52514, 47, 15, 51)\n",
    "\n",
    "timestamps = common_timestamps # List of timestamps (length 52514)\n",
    "\n",
    "# Reshape the tensor\n",
    "reshaped_tensor = np.reshape(tensor, (tensor_shape[0], tensor_shape[1], -1))\n",
    "\n",
    "# Initialize an empty list to store individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over lat and lon combinations\n",
    "for lat_index in tqdm(range(tensor_shape[2])):\n",
    "    for lon_index in range(tensor_shape[3]):        \n",
    "        # Create a DataFrame for the current lat-lon combination\n",
    "        df = pd.DataFrame(reshaped_tensor[:, :, (lat_index * tensor_shape[3]) + lon_index])\n",
    "\n",
    "        df['time'] = timestamps\n",
    "        lat_loc = new_lats_dict[str(lat_index)]\n",
    "        lon_loc = new_lons_dict[str(lon_index)]\n",
    "        df['lat_lon_coord'] = str((float(lat_loc),float(lon_loc)))\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all the individual DataFrames into one big DataFrame\n",
    "big_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = (big_df.set_axis(channels46 +['stn_mesm','time','lat_lon_coord'],axis=1 )\n",
    "      .drop('pm2p5',axis=1)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af27379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('lat_lon_coord')\n",
    "\n",
    "# Creating new columns with previous row values for each group\n",
    "for column in df.drop(['stn_mesm','time','lat_lon_coord'],axis=1).columns:\n",
    "    new_column_name = column + '_prev'\n",
    "    df[new_column_name] = grouped[column].shift()\n",
    "    \n",
    "df['week'] = df['time'].dt.strftime('%W')\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['month'] = df['time'].dt.month\n",
    "df['year'] = df['time'].dt.year\n",
    "df['sin'] = np.sin(df['time'].dt.dayofyear * 2 * np.pi / 365)\n",
    "df['cos'] = np.cos(df['time'].dt.dayofyear * 2 * np.pi / 365)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "136f12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_delta_columns(df):\n",
    "    channels_used =  [\"Q250\",\"Q500\",\"Q700\",\"Q850\",\"Q900\",\n",
    "            \"T250\",\"T500\",\"T700\",\"T850\",\"T900\",\n",
    "            \"U250\",\"U500\",\"U700\",\"U850\",\"U900\",\n",
    "            \"V250\",\"V500\",\"V700\",\"V850\",\"V900\",\n",
    "            \"Z250\",\"Z500\",\"Z700\",\"Z850\",\"Z900\",\n",
    "            \"PV250\",\"PV500\",\"PV700\",\"PV850\",\"PV900\",\n",
    "            \"OMEGA250\",\"OMEGA500\",\"OMEGA700\",\"OMEGA850\",\"OMEGA900\",\n",
    "            \"SLP\",\"u10\",\"v10\",\"duaod550\",'aermr06_50','aermr06_40','aermr06_30', 'aermr06_20',\n",
    "              \"tcwv\"]\n",
    "\n",
    "    # Loop over the channels names\n",
    "    for col in channels_used:\n",
    "        \n",
    "        # Calculate the delta between the column and its corresponding \"_prev\" column\n",
    "        delta_col = col + '_delta'\n",
    "        prev_col = col + '_prev'\n",
    "        df[delta_col] = df[col] - df[prev_col]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = add_delta_columns(df)\n",
    "df = df.drop(['pm10_prev'],axis=1)\n",
    "df['target'] = df['stn_mesm'] - df['pm10']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-ds-env",
   "language": "python",
   "name": "geo-ds-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
